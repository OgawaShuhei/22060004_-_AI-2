#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Mermaidå›³è¡¨ã‚’ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ç”Ÿæˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import os
import requests
import base64
from pathlib import Path

def mermaid_to_image(mermaid_code, filename, output_format="png"):
    """
    Mermaidã‚³ãƒ¼ãƒ‰ã‚’ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›
    
    Args:
        mermaid_code (str): Mermaidè¨˜æ³•ã®ã‚³ãƒ¼ãƒ‰
        filename (str): å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰
        output_format (str): å‡ºåŠ›å½¢å¼ï¼ˆpng, svg, pdfï¼‰
    """
    
    # Mermaid Live Editor APIã‚’ä½¿ç”¨
    mermaid_url = "https://mermaid.ink/img/"
    
    # Mermaidã‚³ãƒ¼ãƒ‰ã‚’Base64ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
    encoded_code = base64.b64encode(mermaid_code.encode('utf-8')).decode('utf-8')
    
    # ç”»åƒURLã‚’æ§‹ç¯‰
    image_url = f"{mermaid_url}{encoded_code}.{output_format}"
    
    try:
        # ç”»åƒã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        response = requests.get(image_url, timeout=30)
        response.raise_for_status()
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
        output_dir = Path("images")
        output_dir.mkdir(exist_ok=True)
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        output_path = output_dir / f"{filename}.{output_format}"
        with open(output_path, 'wb') as f:
            f.write(response.content)
        
        print(f"âœ… {filename}.{output_format} ã‚’ç”Ÿæˆã—ã¾ã—ãŸ: {output_path}")
        return str(output_path)
        
    except Exception as e:
        print(f"âŒ {filename} ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
        return None

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    
    # ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆå›³ã®Mermaidã‚³ãƒ¼ãƒ‰
    diagrams = {
        "system_architecture": """graph TB
    subgraph "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰"
        UI[Streamlit UI]
    end
    
    subgraph "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰"
        MA[ãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒª<br/>app.py]
        TA[ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ<br/>text_analyzer.py]
        UA[URLåˆ†æ<br/>url_analyzer.py]
        DM[ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†<br/>database_manager.py]
    end
    
    subgraph "ãƒ‡ãƒ¼ã‚¿å±¤"
        DB[(SQLite DB)]
        FS[ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ]
    end
    
    subgraph "å¤–éƒ¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª"
        NLTK[NLTK<br/>è‡ªç„¶è¨€èªå‡¦ç†]
        TB[TextBlob<br/>æ„Ÿæƒ…åˆ†æ]
        LD[langdetect<br/>è¨€èªæ¤œå‡º]
        BS[BeautifulSoup4<br/>HTMLè§£æ]
        REQ[Requests<br/>HTTPé€šä¿¡]
    end
    
    subgraph "å¤–éƒ¨ã‚µãƒ¼ãƒ“ã‚¹"
        WEB[Webãƒšãƒ¼ã‚¸]
    end
    
    UI --> MA
    MA --> TA
    MA --> UA
    MA --> DM
    TA --> NLTK
    TA --> TB
    TA --> LD
    UA --> REQ
    UA --> BS
    UA --> WEB
    DM --> DB
    DM --> FS""",

        "data_flow": """flowchart TD
    A[ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›] --> B{å…¥åŠ›ã‚¿ã‚¤ãƒ—}
    
    B -->|ãƒ†ã‚­ã‚¹ãƒˆ| C[ãƒ†ã‚­ã‚¹ãƒˆåˆ†æ]
    B -->|ãƒ•ã‚¡ã‚¤ãƒ«| D[ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿]
    B -->|URL| E[URLåˆ†æ]
    
    D --> C
    E --> F[Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°]
    F --> G[ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º]
    G --> C
    
    C --> H[åˆ†æå‡¦ç†]
    H --> I[åŸºæœ¬çµ±è¨ˆ]
    H --> J[è¨€èªæ¤œå‡º]
    H --> K[æ„Ÿæƒ…åˆ†æ]
    H --> L[å¯èª­æ€§åˆ†æ]
    H --> M[å˜èªé »åº¦åˆ†æ]
    
    I --> N[çµæœçµ±åˆ]
    J --> N
    K --> N
    L --> N
    M --> N
    
    N --> O[ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜]
    N --> P[çµæœè¡¨ç¤º]
    
    O --> Q[å±¥æ­´ç®¡ç†]
    P --> R[ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª]""",

        "class_diagram": """classDiagram
    class TextAnalyzer {
        +analyze_text(text: str) dict
        +get_basic_stats(text: str) dict
        +detect_language(text: str) str
        +analyze_sentiment(text: str) dict
        +calculate_readability(text: str) dict
        +analyze_word_frequency(text: str) dict
        +analyze_sentences(text: str) dict
        +analyze_characters(text: str) dict
    }
    
    class URLAnalyzer {
        +extract_text_from_url(url: str) dict
        +validate_url(url: str) bool
        +scrape_webpage(url: str) str
        +clean_html_content(html: str) str
    }
    
    class DatabaseManager {
        +init_database()
        +save_analysis_result(data: dict) bool
        +get_all_results() list
        +search_results(query: str) list
        +delete_result(result_id: int) bool
        +export_to_csv() str
        +export_to_txt() str
        +get_statistics() dict
    }
    
    class StreamlitApp {
        +main()
        +home_page()
        +text_analysis_page()
        +url_analysis_page()
        +history_page()
        +statistics_page()
    }
    
    StreamlitApp --> TextAnalyzer
    StreamlitApp --> URLAnalyzer
    StreamlitApp --> DatabaseManager""",

        "database_design": """erDiagram
    ANALYSIS_RESULTS {
        int id PK
        text text_content
        text file_name
        text url
        text language
        float sentiment_score
        float subjectivity_score
        float readability_score
        int char_count
        int word_count
        int sentence_count
        int paragraph_count
        text word_frequency
        text sentence_lengths
        text char_frequency
        datetime timestamp
    }
    
    ANALYSIS_RESULTS ||--o{ EXPORT_HISTORY : "generates"
    
    EXPORT_HISTORY {
        int id PK
        int analysis_id FK
        text export_type
        text file_path
        datetime export_time
    }"""
    }
    
    print("ğŸš€ ã‚·ã‚¹ãƒ†ãƒ è¨­è¨ˆå›³ã®ç”»åƒç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
    
    # å„å›³è¡¨ã‚’ç”»åƒã¨ã—ã¦ç”Ÿæˆ
    generated_files = []
    for name, mermaid_code in diagrams.items():
        print(f"\nğŸ“Š {name} ã‚’ç”Ÿæˆä¸­...")
        file_path = mermaid_to_image(mermaid_code, name, "png")
        if file_path:
            generated_files.append(file_path)
    
    print(f"\nğŸ‰ å®Œäº†ï¼ {len(generated_files)}å€‹ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
    print("ğŸ“ ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã¯ 'images/' ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚")
    
    return generated_files

if __name__ == "__main__":
    main() 